{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thaleslsilva/DataScience/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYS_0HIhE0tc"
      },
      "source": [
        "# TF-IDF Para Identificação das Palavras mais Relevantes em um Livro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcAdjzemE0td"
      },
      "source": [
        "TF-IDF significa \"Frequência do Termo - Frequência Inversa de Documentos\". \n",
        "\n",
        "Essa é uma técnica para quantificar uma palavra nos documentos; geralmente calculamos um peso para cada palavra, o que significa a importância da palavra no documento e no corpus. Este método é uma técnica amplamente usada em Recuperação de Informação e Mineração de Texto.\n",
        "\n",
        "Se eu lhe der uma frase, por exemplo, \"Este edifício é tão alto\". É fácil para nós entender a sentença como conhecemos a semântica das palavras e da sentença. Mas como o computador entenderá essa frase? O computador pode entender qualquer dado apenas na forma de valor numérico. Portanto, por esse motivo, vetorizamos todo o texto para que o computador possa entender melhor o texto.\n",
        "\n",
        "Ao vetorizar os documentos, podemos executar várias tarefas, como encontrar documentos relevantes, classificação, agrupamento e assim por diante. É a mesma coisa que acontece quando você realiza uma pesquisa no Google. As páginas da web são chamadas de documentos e o texto com o qual você pesquisa é chamado de consulta. o Google mantém uma representação fixa para todos os documentos. Quando você pesquisa com uma consulta, o Google encontra a relevância da consulta com todos os documentos, classifica-os na ordem em que é relevante e mostra os principais documentos. Todo esse processo é feito usando a forma vetorizada da consulta e dos documentos. Embora os algoritmos do Google sejam altamente sofisticados e otimizados, essa é a estrutura usada.\n",
        "\n",
        "Vamos extrair um livro inteiro, construir nossas funções para TF e IDF e então identificar as palavras mais relevantes em algumas frases do livro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVnFT3dAE0tf"
      },
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install torch==1.5.0\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark. \n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VATLxMrE0th"
      },
      "source": [
        "# Imports\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuWf0PH5E0ti"
      },
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Thales de Lima Silva\" --iversions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuRG1cBGE0tk"
      },
      "source": [
        "### Preparando os Dados\n",
        "\n",
        "https://www.gutenberg.org/files/158/158-h/158-h.htm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8w4Dw9cE0tl"
      },
      "source": [
        "# Carrega os dados\n",
        "nltk.download('gutenberg')\n",
        "dados_livro_emma = nltk.corpus.gutenberg.sents('austen-emma.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLMm-c8fE0tm"
      },
      "source": [
        "# Listas para receber as frases e as palavras do texto\n",
        "dados_livro_emma_frases = []\n",
        "dados_livro_emma_palavras = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaeR52WrE0tt"
      },
      "source": [
        "# Loop para a tokenização\n",
        "# \"isalpha\" retorna cada palavra dentro de uma mesma frase e então passa para a frase seguinte (https://docs.python.org/3/library/stdtypes.html)\n",
        "nltk.download('punkt')\n",
        "for sentence in dados_livro_emma:\n",
        "    dados_livro_emma_frases.append([word.lower() for word in sentence if word.isalpha()])\n",
        "    for word in sentence:\n",
        "        if word.isalpha():\n",
        "            dados_livro_emma_palavras.append(word.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqKxmBQbE0tu"
      },
      "source": [
        "# Vamos converter a lista de palavras em um conjunto (set)\n",
        "dados_livro_emma_palavras = set(dados_livro_emma_palavras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rDVkFEQE0tv"
      },
      "source": [
        "# Visualiza as frases\n",
        "dados_livro_emma_frases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMBJR65E0ty"
      },
      "source": [
        "# Visualiza as palavras\n",
        "dados_livro_emma_palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROtIXn2IE0tz"
      },
      "source": [
        "### Frequência do Termo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB2fJouyE0t1"
      },
      "source": [
        "A Frequência do Termo mede a frequência de uma palavra em um documento. \n",
        "\n",
        "Isso depende muito do tamanho do documento e da generalidade da palavra, por exemplo, uma palavra muito comum como \"era\" pode aparecer várias vezes em um documento, mas se pegarmos dois documentos, um com 100 palavras e outro com 10.000, há uma alta probabilidade de que uma palavra comum como \"era\" possa estar mais presente no documento de 10.000 palavras. Mas não podemos dizer que o documento mais longo é mais importante que o documento mais curto. Por esse exato motivo, realizamos uma normalização no valor da frequência, dividindo a frequência com o número total de palavras no documento.\n",
        "\n",
        "Lembre-se de que precisamos finalmente vetorizar o documento. Quando estamos planejando vetorizá-lo, não podemos considerar apenas as palavras que estão presentes nesse documento específico. Se fizermos isso, o comprimento do vetor será diferente para os dois documentos e não será possível calcular a semelhança. Então, o que fazemos é que vetorizar os documentos no vocabulário, que é a lista de todas as palavras possíveis no corpus.\n",
        "\n",
        "Quando estamos vetorizando os documentos, verificamos a contagem de cada palavra. Na pior das hipóteses, se o termo não existir no documento, esse valor de TF específico será 0 e, em outro caso extremo, se todas as palavras no documento forem iguais, será 1. O valor final do documento normalizado estará no intervalo de [0 a 1], sendo 0, 1 inclusive.\n",
        "\n",
        "**tf(t,d) = contagem de t em d / número de palavras em d**\n",
        "\n",
        "Se já calculamos o valor do TF e se isso produz uma forma vetorizada do documento, por que não usar apenas o TF para encontrar a relevância entre os documentos? Por que precisamos da IDF?\n",
        "\n",
        "Embora tenhamos calculado o valor do TF, ainda existem alguns problemas, por exemplo, palavras mais comuns como \"é\" terão valores muito altos, dando a essas palavras uma importância muito alta. Mas usar essas palavras para calcular a relevância produz maus resultados. \n",
        "\n",
        "Esse tipo de palavra comum é chamado de palavras de parada (stop words) e, embora removamos as stop words posteriormente na etapa de pré-processamento, descobrir a importância da palavra em todos os documentos e normalizando  esse valor, representa muito melhor os documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyQqq25xE0t3"
      },
      "source": [
        "# Função para calcular a Termo Frequência\n",
        "def TermFreq(documento, palavra):\n",
        "    doc_length = len(documento)\n",
        "    ocorrencias = len([w for w in documento if w == palavra])\n",
        "    return ocorrencias / doc_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_K3Wd2YE0t4"
      },
      "source": [
        "dados_livro_emma_frases[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cNWjTPbE0t6"
      },
      "source": [
        "# Aplica a função\n",
        "TermFreq(dados_livro_emma_frases[5], 'mother')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnWic3B4E0t8"
      },
      "source": [
        "Podemos então criar um dicionário (vocabulário).\n",
        "\n",
        "Cada palavra única será identificada de forma única no objeto de dicionário. Isso é necessário para criar representações de textos. O corpus Bag of Words é criado e será necessário para a construção do modelo TF-IDF. \n",
        "\n",
        "O dicionário é criado pela lista de palavras. As frases/documentos, etc., podem ser convertidos em uma lista de palavras e depois alimentados nos corpora como parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm9QDAq-E0t9"
      },
      "source": [
        "# Criamos um corpus Bag of words\n",
        "def cria_dict():\n",
        "    output = {}\n",
        "    for word in dados_livro_emma_palavras:\n",
        "        output[word] = 0\n",
        "        for doc in dados_livro_emma_frases:\n",
        "            if word in doc:\n",
        "                output[word] += 1\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuc6YDwAE0t-"
      },
      "source": [
        "# Cria o dicionário\n",
        "df_dict = cria_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX-V-F96E0t_"
      },
      "source": [
        "# Filtra o dicionário\n",
        "df_dict['mother']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGmFTXzME0uA"
      },
      "source": [
        "### Frequência Inversa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQEkflPuE0uB"
      },
      "source": [
        "Para compreender o que é IDF, primeiro temos que compreender o que é DF.\n",
        "\n",
        "A **Frequência do Documento** (DF) mede a importância do documento em todo o corpus e é muito semelhante ao TF. A única diferença é que TF é contador de frequência para um termo t no documento d, onde DF é a contagem de ocorrências do termo t no conjunto de documentos N. \n",
        "\n",
        "Em outras palavras, DF é o número de documentos em que a palavra está presente. Consideramos uma ocorrência se o termo consistir no documento pelo menos uma vez, não precisamos saber o número de vezes que o termo está presente.\n",
        "\n",
        "df (t) = ocorrência de t nos documentos\n",
        "\n",
        "Para manter isso também em um intervalo, normalizamos dividindo com o número total de documentos. Nosso principal objetivo é conhecer a informatividade de um termo, e DF é o inverso exato dele. É por isso que invertemos o DF.\n",
        "\n",
        "**Frequência Inversa de Documentos**\n",
        "\n",
        "IDF é o inverso da frequência do documento que mede a informatividade do termo t. Quando calcularmos o IDF, será muito baixo para as palavras que mais ocorrem, como stop words (porque stop words como \"é\" estão presentes em quase todos os documentos e N / df atribuirá um valor muito baixo a essa palavra). Isso finalmente resulta o que queremos, uma ponderação relativa.\n",
        "\n",
        "idf (t) = N / df\n",
        "\n",
        "Agora, existem alguns outros problemas com o IDF, no caso de um corpus grande, digamos 10.000, o valor do IDF explode. \n",
        "\n",
        "Então, para diminuir o efeito, aplicamos o log ao IDF.\n",
        "\n",
        "Durante o tempo de consulta, quando uma palavra que não está no vocabulário ocorre, o df será 0. Como não podemos dividir por 0, suavizamos o valor adicionando 1 ao denominador.\n",
        "\n",
        "idf (t) = log (N / (df + 1))\n",
        "\n",
        "Finalmente, considerando um valor multiplicativo de TF e IDF, obtemos a pontuação TF-IDF, existem muitas variações diferentes de TF-IDF, mas por enquanto vamos nos concentrar nessa versão básica.\n",
        "\n",
        "**tf-idf (t, d) = tf (t, d) * log (N / (df + 1))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkbEKYuJE0uC"
      },
      "source": [
        "# Função para calcular a Frequência Inversa de Documentos\n",
        "def InverseDocumentFrequency(word):\n",
        "    N = len(dados_livro_emma_frases)\n",
        "    try:\n",
        "        df = df_dict[word] + 1\n",
        "    except:\n",
        "        df = 1\n",
        "    return np.log(N/df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbrWs4rKE0uJ"
      },
      "source": [
        "# Aplica a função\n",
        "InverseDocumentFrequency('mother')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I6hab-BE0uO"
      },
      "source": [
        "### TF/IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyDeiP1RE0uP"
      },
      "source": [
        "# Função TF-IDF\n",
        "def TFIDF(doc, word):\n",
        "    tf = TermFreq(doc, word)\n",
        "    idf = InverseDocumentFrequency(word)\n",
        "    return tf * idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XoILyjLE0uP"
      },
      "source": [
        "dados_livro_emma_frases[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-nULupJE0uR"
      },
      "source": [
        "# Print\n",
        "print('mother: ' + str(TFIDF(dados_livro_emma_frases[5], 'mother')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_xVIJYvE0uS"
      },
      "source": [
        "dados_livro_emma_frases[30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72gRYqccE0uT"
      },
      "source": [
        "# Print\n",
        "print('mother: ' + str(TFIDF(dados_livro_emma_frases[30], 'mother')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5DN3lDSE0uU"
      },
      "source": [
        "# Fim"
      ]
    }
  ]
}
