{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Word2vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thaleslsilva/DataScience/blob/master/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV6J211OQSKg"
      },
      "source": [
        "# Estudo de Caso - Previsão de Palavras com Base no Contexto e Visualização com PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4TARMhtQSKi"
      },
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install torch==1.5.0\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark. \n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfQH-eCVQSKi"
      },
      "source": [
        "# Instala o PyTorch\n",
        "!pip install -q torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCL8DlfNQSKj"
      },
      "source": [
        "# Pacote para gráficos com Scikit-learn\n",
        "!pip install -q scikit-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNjKKJ76QSKk"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import scipy\n",
        "import sklearn\n",
        "import scikitplot\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from torch.autograd import Variable, profiler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import cosine\n",
        "from scikitplot.decomposition import plot_pca_2d_projection\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqsGyxHiQSKl"
      },
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Thales de Lima Silva\" --iversions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYO6uN-KQSKm"
      },
      "source": [
        "### Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLq5fzZ_QSKn"
      },
      "source": [
        "# Corpus\n",
        "corpus = ['ele é um rei',\n",
        "          'ela é uma rainha',\n",
        "          'ele é um homem',\n",
        "          'ela é uma mulher',\n",
        "          'Madrid é a capital da Espanha',\n",
        "          'Berlim é a capital da Alemanha',\n",
        "          'Lisboa é a capital de Portugal']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSrvlELWQSKp"
      },
      "source": [
        "# Construindo o vocabulário com tokenização\n",
        "palavras = []\n",
        "for sentence in corpus:\n",
        "    for palavra in sentence.split():\n",
        "         if palavra not in palavras:\n",
        "            palavras.append(palavra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ossk96lcQSKq"
      },
      "source": [
        "# Visualiza os dados\n",
        "palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTHa1iqQSKt"
      },
      "source": [
        "# Criamos o mapeamento palavra - índice   \n",
        "word2idx = {w:idx for (idx, w) in enumerate(palavras)}\n",
        "word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jty1ms57QSKu"
      },
      "source": [
        "# Criamos o mapeamento inverso índice - palavra\n",
        "idx2word = {idx:w for (idx, w) in enumerate(palavras)}\n",
        "idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyGp7BivQSKv"
      },
      "source": [
        "# Tamanho do vocabulário\n",
        "tamanho_vocab = len(word2idx)\n",
        "tamanho_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oaYNkV0QSKw"
      },
      "source": [
        "### Construção do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saQgm8wzQSKx"
      },
      "source": [
        "# Função para gerar os embeddings\n",
        "def get_word_embedding(word):\n",
        "    word_vec_one_hot = np.zeros(tamanho_vocab)\n",
        "    word_vec_one_hot[word2idx[word]] = 1\n",
        "    return word_vec_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xXXkc9uQSKy"
      },
      "source": [
        "# Função para gerar os vetores, da palavra central e do contexto\n",
        "def gera_vetores():\n",
        "    for sentence in corpus:\n",
        "        words = sentence.split()\n",
        "        indices = [word2idx[w] for w in words]\n",
        "        \n",
        "        # Loop pelo range de índices\n",
        "        # Aqui geramos o vetor da palavra central em i\n",
        "        # E geramos o vetor de contexto\n",
        "        for i in range(len(indices)):\n",
        "            for w in range(-window_size, window_size + 1):\n",
        "                context_idx = i + w\n",
        "                if context_idx < 0 or context_idx >= len(indices) or i == context_idx:\n",
        "                    continue\n",
        "                    \n",
        "                # Gera os vetores    \n",
        "                center_vec_one_hot = np.zeros(tamanho_vocab)\n",
        "                center_vec_one_hot[indices[i]] = 1\n",
        "                context_idx = indices[context_idx]\n",
        "                                \n",
        "                yield center_vec_one_hot, context_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bcLw97eQSKz"
      },
      "source": [
        "# Hiperparâmetros\n",
        "embedding_dims = 10\n",
        "window_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFlSMPWQSKz"
      },
      "source": [
        "Definição dos pesos da rede neural.\n",
        "\n",
        "- W1 é uma matriz de pesos de dimensões embedding_dims x tamanho_vocab\n",
        "- W2 é uma matriz de pesos de dimensões tamanho_vocab x embedding_dims\n",
        "\n",
        "Os pesos (ou coeficientes ou parâmetros) é aquilo que a rede aprende durante o treinamento. Como no início não sabemos qual o valor ideal de pesos (isso é o que queremos descobrir) iniciamos com valores randômicos usando torch.randn().\n",
        "\n",
        "Ao final do aprendizado, o modelo em si nada mais é do que os valores ideais de W1 e W2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Nt68kVpoQSK0"
      },
      "source": [
        "# Definição dos pesos da rede neural\n",
        "W1 = Variable(torch.randn(embedding_dims, tamanho_vocab).float(), requires_grad = True)\n",
        "W2 = Variable(torch.randn(tamanho_vocab, embedding_dims).float(), requires_grad = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3J10i2QSK1"
      },
      "source": [
        "# Treinamento\n",
        "print(\"\\nIniciando o Treinamento...\\n\")\n",
        "for epoch in range(1001):\n",
        "    \n",
        "    # Inicializa o erro médio da rede\n",
        "    avg_loss = 0\n",
        "    \n",
        "    # Inicializa o controle do número de amostras\n",
        "    samples = 0\n",
        "    \n",
        "    # Loop pelos dados (vetores de entrada)\n",
        "    for data, target in gera_vetores():\n",
        "        \n",
        "        # Coleta x (vetor da palavra central)\n",
        "        x = Variable(torch.from_numpy(data)).float()\n",
        "        \n",
        "        # Coleta y (vetor do contexto)\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "        \n",
        "        # Atualiza o número de amostras\n",
        "        samples += len(y_true)\n",
        "        \n",
        "        # Resultado da multiplicação entre os pesos e as primeiras camadas da rede\n",
        "        a1 = torch.matmul(W1, x)\n",
        "        a2 = torch.matmul(W2, a1)\n",
        "\n",
        "        # A função softmax entrega a probabilidade da previsão da rede\n",
        "        log_softmax = F.log_softmax(a2, dim = 0)\n",
        "\n",
        "        # Previsão da rede\n",
        "        network_pred_dist = F.softmax(log_softmax, dim = 0)\n",
        "        \n",
        "        # Calcula o erro, comparando a previsão da rede com o valor real \n",
        "        # (como fazemos em qualquer modelo de aprendizagem supervisionada)\n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        # Erro médio\n",
        "        avg_loss += loss.item()\n",
        "        \n",
        "        # Inicia o backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Atualiza o valor dos pesos para a próxima passada\n",
        "        W1.data -= 0.002 * W1.grad.data\n",
        "        W2.data -= 0.002 * W2.grad.data\n",
        "\n",
        "        # Zera o valor do gradiente depois de atualizar os pesos\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "    # Imprime o erro da rede\n",
        "    if epoch % 10 == 0:\n",
        "        print('Erro de Treinamento:', avg_loss / samples)\n",
        "\n",
        "print(\"\\nTreinamento Concluído.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-bTp9EUQSK3"
      },
      "source": [
        "### Teste do Modelo e Redução de Dimensionalidade com PCA\n",
        "\n",
        "Para testar o modelo, tudo que precisamos é dos pesos, em nosso exemplo W1 e W2. Mas visualizar os dados é desafiador, pois a dimensionalidade é alta e quanto maior o número de palavras do vocabulário, mais complicado.\n",
        "\n",
        "Uma alternativa, é reduzir a diemensionalidade dos dados. Convertemos todos os atributos em 2 componentes principais usando PCA (Principal Component Analysis) e com 2 componentes podemos visualizar os dados.\n",
        "\n",
        "Cada componentes principal nada mais é do que a junção matemática da informação em outras variáveis. O PCA é um algoritmo de Machine Learning por si mesmo, da categoria de aprendizagem não supervisionada.\n",
        "\n",
        "Vamos aplicar o PCA para visualizar os dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l3EADUorQSK5"
      },
      "source": [
        "# Cria o objeto para redução de dimensionalidade\n",
        "pca = PCA(n_components = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiJ3UQUgQSK6"
      },
      "source": [
        "# Treina o modelo PCA\n",
        "pca.fit(W1.data.numpy().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CGu794TQSK7"
      },
      "source": [
        "# Calcula a projeção PCA para o Plot\n",
        "proj = pca.transform(W1.data.numpy().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlI-XN6yQSK9"
      },
      "source": [
        "# Plot\n",
        "ax = plot_pca_2d_projection(pca, \n",
        "                            W1.data.numpy().T, \n",
        "                            np.array(palavras), \n",
        "                            feature_labels = palavras, \n",
        "                            figsize = (16,10), \n",
        "                            text_fontsize = 12)\n",
        "\n",
        "# Legenda\n",
        "for i, txt in enumerate(palavras):\n",
        "    ax.annotate(txt, (proj[i,0], proj[i,1]), size = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEzKgle1QSK-"
      },
      "source": [
        "Observe a legenda no gráfico acima! Palavras similares com base no contexto, estão com a \"bolinha\" com cores parecidas. No topo da lista temos países e cidades, depois pronomes e a palavra \"capital\", temos então homem, mulher, rainha e rei e por fim artigos e um verbo.\n",
        "\n",
        "Tudo isso foi aprendido pela rede com base no contexto, que nada mais é do que a distância de cosseno entre as embeddings, os vetores que representam as palavras.\n",
        "\n",
        "A visualização acima mostra que palavras que estão na mesma direção possui alguma similaridade, por exemplo \"Alemanha\" e \"Berlim\". Passe uma linha reta imaginária que \"corta\" as palavras \"Alemanha\" e \"Berlim\". Consegue? Se a resposta for sim, as palavras são similares. Abaixo terá outro exemplo.\n",
        "\n",
        "Vamos extrair as distâncias com base na pergunta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ON3qzjdQSK_"
      },
      "source": [
        "**Espanha está para Madrid, assim como Alemanha está para ?**\n",
        "\n",
        "Vamos perguntar ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC4fgBALQSLE"
      },
      "source": [
        "# Função para obter um vetor de palavras no peso W1 (esse é o contexto)\n",
        "def get_word_vector_v(word):\n",
        "    return W1[:, word2idx[word]].data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpSDvN-eQSLF"
      },
      "source": [
        "# Função para obter um vetor de palavras no peso W2 (essa é a palavra central)\n",
        "def get_word_vector_u(word):\n",
        "    return W2[word2idx[word],:].data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFptFVCQSLG"
      },
      "source": [
        "# Vamos obter os vetores das palavras\n",
        "espanha = 1 * get_word_vector_v('Espanha') + 1 * get_word_vector_u('Espanha')\n",
        "alemanha = 1 * get_word_vector_v('Alemanha') + 1 * get_word_vector_u('Alemanha') \n",
        "madrid = 1 * get_word_vector_v('Madrid') + 1 * get_word_vector_u('Madrid') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86JlHtnOQSLG"
      },
      "source": [
        "# Resultado\n",
        "resultado = madrid - espanha + alemanha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_-JRZuwQSLI"
      },
      "source": [
        "# Este é o resultado, ou seja, uma embedding que representa a palavra mais similar à palavra \"Alemanha\",\n",
        "# com base na similaridade (contexto) entre \"Polônia\" e \"Varsóvia\".\n",
        "resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZFezr1XQSLJ"
      },
      "source": [
        "# Vamos extrair as distâncias de todas as outras palavras para a nossa palavra \"secreta\" que está \n",
        "# no vetor embedding chamado \"resultado\"\n",
        "# Usamos a função cosine() do SciPy para calcular as distâncias\n",
        "distancias = [(v, cosine(resultado, 1 * get_word_vector_u(v) + 1 * get_word_vector_v(v))) for v in palavras]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9fk2hxBQSLL"
      },
      "source": [
        "# Visualiza as distâncias\n",
        "distancias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu9OCyKOQSLN"
      },
      "source": [
        "Acima temos uma lista de tuplas com as distâncias de cada palavra para nosso \"resultado\". Vamos ordenar isso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QObP4C2vQSLO"
      },
      "source": [
        "# Ordenando a lista de tuplas pelo segundo elemento da tupla\n",
        "distancias.sort(key = lambda tup: tup[1])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjJIg6HJQSLP"
      },
      "source": [
        "# Agora sim\n",
        "distancias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4AxYrHxQSLQ"
      },
      "source": [
        "O vetor \"resultado\" foi uma previsão do nosso modelo e as palavras \"Madrid\" e \"Berlim\" são as mais similares. Observe que \"Berlim\" é a palavra mais similar com base no conexto, uma vez que Madrid já foi usada em nossa fórmula.\n",
        "\n",
        "Imagine que um vetor (uma flecha) sai da origem do sistema de coordenadas (Honestidade = 0 e Experiência = 0, chamaremos de ponto O) e termina no ponto X. Este vetor é usado para localizar o ponto no nosso espaço de características. Não é diferente de simplesmente dizer que X possui H = 0.4 e E = 0.2, é apenas outra maneira de ver isso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7zNHD_GQSLQ"
      },
      "source": [
        "**Em que contexto aparece a palavra Lisboa?**\n",
        "\n",
        "Aqui é como se estivéssemos usando o modelo para previsão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfT2XmhLQSLR"
      },
      "source": [
        "# Extrai o contexto\n",
        "context_to_predict = get_word_vector_v('Lisboa')\n",
        "\n",
        "# Variável com o contexto a prever\n",
        "hidden = Variable(torch.from_numpy(context_to_predict)).float()\n",
        "\n",
        "# Executa o modelo e extrai as probabilidades \n",
        "# (executar o modelo nada mais é do que multiplicar os novos dados de entrada pelos pesos aprendidos no treinamento)\n",
        "a = torch.matmul(W2, hidden)\n",
        "probs = F.softmax(a, dim = 0).data.numpy()\n",
        "\n",
        "# Imprime o resultado\n",
        "for context, prob in zip(palavras, probs):\n",
        "    print(f'{context}: {prob}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ut9JSlQSLT"
      },
      "source": [
        "O contexto da palavra \"Lisboa\" é representado pelas palavras \"é\", \"a\", \"Portugal\".\n",
        "\n",
        "Nosso modelo não conseguiu aprender o contexto \"capital\". Quem sabe você consegue otimizar o treinamento do modelo e aumentar sua precisão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goDoVRgMQSLT"
      },
      "source": [
        "# Fim"
      ]
    }
  ]
}
