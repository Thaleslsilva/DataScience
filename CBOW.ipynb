{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CBOW.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thaleslsilva/DataScience/blob/master/CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqUfNisJDWA"
      },
      "source": [
        "# Estudo de Caso - Inteligência Artificial Para Previsão de Sentenças em Embargos de Declaração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xsgdRbKJDWD"
      },
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install torch==1.5.0\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark. \n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX26AkzNJDWF"
      },
      "source": [
        "# Instala o PyTorch\n",
        "!pip install -q torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "658joMmqJDWH"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTse9M0mJDWI"
      },
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Thales de Lima Silva\" --iversions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjvgnzCJDWM"
      },
      "source": [
        "### Preparando os Dados\n",
        "\n",
        "O texto abaixo é um exemplo de embargo de declaração. Embora o texto represente um embargo, dados críticos foram substituídos por informações genéricas, o que não compromete o objetivo do estudo de caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB6gnRmUJDWN"
      },
      "source": [
        "# Texto de embargo de declaração\n",
        "embargo = \"\"\"O embargante sofreu o ajuizamento de ação de danos morais e materiais, cujo objeto é o reaver os \n",
        "valores pagos pelo sinal dado em um contrato de compra e venda de imóvel no qual não foi dado continuidade. Em 24 \n",
        "de fevereiro de 2012, o Magistrado proferiu decisão de fls. 277 a 280, que condenou todas as demandadas \n",
        "solidariamente no seguinte teor: Diante de todo o exposto, com fundamento no art. 1234, I, do CPC/2015, \n",
        "julgo procedentes em parte os pedidos constantes na inicial, condenando solidariamente as demandadas, XPTO LTDA, \n",
        "BOB CAMARGO DE MORAES, a Pagarema título de indenização por danos morais, consoante fundamentação acima discorrida, \n",
        "o montante de R$ 1.500,00 (um mil e quinhentos reais), corrigidos monetariamente pelo INPC desde a data \n",
        "desta decisão, acrescidos de juros de 1% ao mês, a partir da citação; condeno ainda, à restituição do valor \n",
        "pago pelo demandante como sinal da entrada do imóvel, descontando apenas 20% (vinte por cento), referente às \n",
        "despesas, devendo incidir juros de 1% (um por cento) ao mês contados da citação e correção monetária pelo INPC a \n",
        "partir da sentença. Contudo, data venia, houve omissão e obscuridade na referida decisão, haja vista que a omissão \n",
        "se deu pela ausência dos julgamentos das preliminares (Necessidade de Perícia Técnica e a incompetência de \n",
        "Juizado Especial) proposta posteriormente em aditamento de contestação (Fls 251 a 254) para impugnar áudios \n",
        "juntados pelo embargado, autorizado a ser realizada pela Douta Magistrada em audiência de Conciliação, \n",
        "instrução e julgamento de fls 235 e 236, por ausência de intimação anterior para realizar a já tratada \n",
        "impugnação aos áudios anexados.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGD_P-AJDWP"
      },
      "source": [
        "# Limpeza do texto substituindo vírgulas e pontos por espaços e colocando as palavras em minúsculo\n",
        "embargo = embargo.replace(',','').replace('.','').lower().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-EDqxOJDWQ"
      },
      "source": [
        "# Criação do corpus com o texto acima\n",
        "corpus = set(embargo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYLe1I2DJDWS"
      },
      "source": [
        "# Visualizamos o corpus\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8vVOw2JDWT"
      },
      "source": [
        "# Comprimento do corpus\n",
        "corpus_length = len(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNfQCVhUJDWX"
      },
      "source": [
        "# Dicionários para TF-IDF\n",
        "dic_palavra = {}\n",
        "dic_inverso_palavra = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07u4MfyRJDWY"
      },
      "source": [
        "# Loop pelo corpus para criar os dicionários\n",
        "for i, palavra in enumerate(corpus):\n",
        "    dic_palavra[palavra] = i\n",
        "    dic_inverso_palavra[i] = palavra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8JdXv5GJDWY"
      },
      "source": [
        "# Lista para receber os dados\n",
        "dados = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE9GjVTrJDWZ"
      },
      "source": [
        "# Loop pelo texto par extrair sentenças e palavras\n",
        "for i in range(2, len(embargo) - 2):\n",
        "    sentence = [embargo[i-2], embargo[i-1], embargo[i+1], embargo[i+2]]\n",
        "    target = embargo[i]\n",
        "    dados.append((sentence, target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvbbiPFfJDWa"
      },
      "source": [
        "Você leu o código acima e compreendeu o que foi feito? Observe esta linha:\n",
        "\n",
        "sentence = [embargo[i-2], embargo[i-1], embargo[i+1], embargo[i+2]]\n",
        "\n",
        "Para uma palavra no índice i, obtemos duas palavras antes e duas palavras depois. A palavra no índice i será o nosso target e a sentença será composta das duas palavras e duas palavras depois da palavra target. \n",
        "\n",
        "Após treinar o modelo, seremos capazes de prever cada palavra com base nas palavras a sua volta.\n",
        "\n",
        "Aqui um exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrX_H3UMJDWa"
      },
      "source": [
        "# Visualiza os dados\n",
        "print(dados[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUjRUCMaJDWb"
      },
      "source": [
        "As quatro palavras na lista serão os dados de entrada e a palavra fora da lista ('de' nesse caso), será a variável de saída."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVbFPX2gJDWc"
      },
      "source": [
        "### Construção do Modelo CBoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQHbyie2JDWc"
      },
      "source": [
        "# Vamos definir o comprimento de cada embedding\n",
        "embedding_length = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e0k2PWkJDWd"
      },
      "source": [
        "# Classe para o modelo\n",
        "class CBoW(torch.nn.Module):\n",
        "\n",
        "    # Método construtor\n",
        "    def __init__(self, corpus_length, embedding_dim):\n",
        "        super(CBoW, self).__init__()\n",
        "        \n",
        "        # Camada de entrada do modelo para criação da embedding\n",
        "        self.embeddings = nn.Embedding(corpus_length, embedding_dim)\n",
        "\n",
        "        # Camadas lineares\n",
        "        self.linear1 = nn.Linear(embedding_dim, 64)\n",
        "        self.linear2 = nn.Linear(64, corpus_length)\n",
        "        \n",
        "        # Camadas de ativação\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "    # Passo (forward)\n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        # Aqui definimos a ordem ds camadas da rede neural\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    # Obtém a word_emdedding\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.LongTensor([dic_palavra[word]])\n",
        "        return self.embeddings(word).view(1,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmRujKonJDWe"
      },
      "source": [
        "# Cria o modelo CBoW\n",
        "modelo = CBoW(corpus_length, embedding_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIr4KZUVJDWf"
      },
      "source": [
        "# Função de custo\n",
        "loss_function = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHyRrunzJDWf"
      },
      "source": [
        "# Otimizador do modelo (backpropagation)\n",
        "optimizer = torch.optim.SGD(modelo.parameters(), lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfHJawvEJDWg"
      },
      "source": [
        "# Função para criar o vetor de sentenças, necessário para treinar o modelo\n",
        "def make_sentence_vector(sentence, word_dict):\n",
        "    idxs = [word_dict[w] for w in sentence]\n",
        "    return torch.tensor(idxs, dtype = torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ZkLMVTJDWg"
      },
      "source": [
        "# Aqui está nosso dicionário de palavras\n",
        "dic_palavra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ji-zqWgJDWh"
      },
      "source": [
        "# O dicionário de palavras será convertido em um vetor de sentenças. Aqui um exemplo:\n",
        "print(make_sentence_vector(['pela','ausência','dos','julgamentos'], dic_palavra))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH0zt0UfJDWi"
      },
      "source": [
        "### Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "OUYnTzhbJDWj"
      },
      "source": [
        "# Loop por 150 passadas (epochs) de treinamento\n",
        "for epoch in range(150):\n",
        "    \n",
        "    # Inicia o erro da época com 0\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # Loop pelos dados de entrada (sentence) e saída (target)\n",
        "    for sentence, target in dados:\n",
        "        \n",
        "        # Inicializa os gradientes com zero\n",
        "        modelo.zero_grad()\n",
        "        \n",
        "        # Cria o vetor de sentença com os dados de entrada (que devem estar no dicionário de palavras)\n",
        "        sentence_vector = make_sentence_vector(sentence, dic_palavra)  \n",
        "        \n",
        "        # Usa o vetor para fazer previsões com o modelo e retorna as probabilidades\n",
        "        log_probs = modelo(sentence_vector)\n",
        "        \n",
        "        # Calcula o erro do modelo\n",
        "        loss = loss_function(log_probs, torch.tensor([dic_palavra[target]], dtype = torch.long))\n",
        "        \n",
        "        # Chama o método de backpropagation para calcular o gradiente da derivada\n",
        "        loss.backward()\n",
        "        \n",
        "        # Otimiza os pesos do modelo e segue para a próxima passada\n",
        "        # É aqui que o aprendizado acontece\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Atualiza o erro da época\n",
        "        epoch_loss += loss.data\n",
        "        \n",
        "    # Imprime epoch e erro da epoch    \n",
        "    print('Epoch: ' + str(epoch) + ', Erro do Modelo: ' + str(epoch_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsyasyFdJDWt"
      },
      "source": [
        "Observe como o erro foi reduzido a cada passada, nitidamente o aprendizado ocorrendo. Vamos agora usar o modelo para fazer previsões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB4njV9BJDWx"
      },
      "source": [
        "# Função para obter uma previsão\n",
        "def get_resultado_previsto(input, dic_inverso_palavra):\n",
        "    index = np.argmax(input)\n",
        "    return dic_inverso_palavra[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-QPfpmQJDWx"
      },
      "source": [
        "# Função para prever sentenças (aplicamos aos novos dados o mesmo tratamento usado nos dados de treino)\n",
        "def preve_sentenca(sentence):\n",
        "    \n",
        "    # Dividimos a sentença com split\n",
        "    sentence_split = sentence.replace('.','').lower().split()\n",
        "    \n",
        "    # Criamos o vetor de sentença\n",
        "    sentence_vector = make_sentence_vector(sentence_split, dic_palavra)\n",
        "    \n",
        "    # Faz a previsão com o modelo\n",
        "    prediction_array = modelo(sentence_vector).data.numpy()\n",
        "    \n",
        "    # Print dos resultados\n",
        "    print('Palavras Anteriores: {}\\n'.format(sentence_split[:2]))\n",
        "    print('Palavra Prevista: {}\\n'.format(get_resultado_previsto(prediction_array[0], dic_inverso_palavra)))\n",
        "    print('Palavras Seguintes: {}\\n'.format(sentence_split[2:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wETPhXxJDWz"
      },
      "source": [
        "### Previsões com o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTxc2SwJDW0"
      },
      "source": [
        "Dentro da frase: **\"ausência de intimação anterior para realizar\"**, vejamos se o modelo consegue prever a palavra.\n",
        "\n",
        "Vou omitir a palavra **intimação** e essa deve ser a palavra prevista pelo modelo. Vamos passar como dados de entrada as duas palavras anteriores e as duas palavras posteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWkuqq4JJDW0"
      },
      "source": [
        "# Previsão com o modelo\n",
        "preve_sentenca('ausência de anterior para')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5uq9j8rJDW1"
      },
      "source": [
        "# Emdedding da palavra\n",
        "print(modelo.get_word_emdedding('intimação'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSMfz1lJDW5"
      },
      "source": [
        "Dentro da frase: **\"devendo incidir juros de 1%\"**, vejamos se o modelo consegue prever a palavra.\n",
        "\n",
        "Vou omitir a palavra **juros** e essa deve ser a palavra prevista pelo modelo. Vamos passar como dados de entrada as duas palavras anteriores e as duas palavras posteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM0PTFJeJDW6"
      },
      "source": [
        "# Previsão com o modelo\n",
        "preve_sentenca('devendo incidir de 1%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ajCV7_LNJDW8"
      },
      "source": [
        "# Fim"
      ]
    }
  ]
}