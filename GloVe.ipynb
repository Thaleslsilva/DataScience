{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "GloVe.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thaleslsilva/DataScience/blob/master/GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxDBU0o3WzYK"
      },
      "source": [
        "# Estudo de Caso - Buscador de Palavras em Texto Por Similaridade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7uTy_W3WzYN"
      },
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install torch==1.5.0\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark. \n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdHyTIZdWzYN"
      },
      "source": [
        "# Instala o PyTorch\n",
        "!pip install -q torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjK21oYFWzYP"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "from nltk.tokenize import word_tokenize\n",
        "%matplotlib inline\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5r7mf48WzYQ"
      },
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Thales de Lima Silva\" --iversions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgB215G1WzYS"
      },
      "source": [
        "### Carregando e Processando os Dados\n",
        "\n",
        "Para este estudo de caso, usaremos o famoso texto de Isaac Asimov: The Last Question.\n",
        "\n",
        "http://users.ece.cmu.edu/~gamvrosi/thelastq.html\n",
        "\n",
        "Traduzimos o texto e usaremos para treinar o modelo GloVe e depois buscar palavras por similaridade. Recomendados a leitura do arquivo asimov.txt (usado na célula abaixo) antes de executar o restante do Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxmxbMofWzYT"
      },
      "source": [
        "# Abre o arquivo para leitura e carrega na variável arquivo_texto\n",
        "arquivo_texto = open('asimov.txt','r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-M7mc2SWzYV"
      },
      "source": [
        "# Converte as palavars para minúsculo\n",
        "texto = arquivo_texto.read().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqkg2Q8IWzYX"
      },
      "source": [
        "# Fecha o arquivo\n",
        "arquivo_texto.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rc0STd_WzYX"
      },
      "source": [
        "# Tokenização do texto\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "texto_token = word_tokenize(texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oebM2bdKWzYY"
      },
      "source": [
        "# Variável para o comprimento total dos tokens\n",
        "comp_tokens = len(texto_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7MqEPRYWzYZ"
      },
      "source": [
        "print(\"Número de Tokens: \", comp_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBS87fuTWzYc"
      },
      "source": [
        "### Criando o Vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL4KfI7XWzYd"
      },
      "source": [
        "# Criando o vocabulário\n",
        "vocab = set(texto_token)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Tamanho do Vocabulário:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvh-K8zNWzYe"
      },
      "source": [
        "# Dicionário para mapear as palavras aos índices\n",
        "palavra_indice = {palavra: i for i, palavra in enumerate(vocab)}\n",
        "palavra_indice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YYKB7zWzYf"
      },
      "source": [
        "# Dicionário para mapear os índices às palavras\n",
        "indice_palavra = {i: palavra for i, palavra in enumerate(vocab)}\n",
        "indice_palavra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzTXM79tWzYh"
      },
      "source": [
        "Salvo indicação em contrário, usamos um contexto de dez palavras à esquerda e dez palavras à direita."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOKB6JjFWzYi"
      },
      "source": [
        "# Tamanho do contexto\n",
        "CONTEXT_SIZE = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExHEWRgTWzYi"
      },
      "source": [
        "# Matriz de co-ocorrência preenchida com zeros\n",
        "co_occ_mat = np.zeros((vocab_size, vocab_size))\n",
        "co_occ_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPIeXiyvWzYk"
      },
      "source": [
        "Agora percorremos os dicionários de mapeamento criados anteriormente e preenchemos a matriz de co-ocorrência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJHnVOvWzYk"
      },
      "source": [
        "# Loop externo por todo comprimento do vocabulário\n",
        "for i in range(comp_tokens):\n",
        "    \n",
        "    # Loop interno pelo tamanho do contexto\n",
        "    for dist in range(1, CONTEXT_SIZE + 1):\n",
        "        \n",
        "        # Obtém o índice do token\n",
        "        ix = palavra_indice[texto_token[i]]\n",
        "        \n",
        "        # Se a palara estiver à esquerda, inserimos à esquerda na matriz de co-ocorrência\n",
        "        if i - dist > 0:\n",
        "            left_ix = palavra_indice[texto_token[i - dist]]\n",
        "            co_occ_mat[ix, left_ix] += 1.0 / dist\n",
        "            \n",
        "        # Se a palara estiver à direita, inserimos à direita na matriz de co-ocorrência\n",
        "        if i + dist < len(texto_token):\n",
        "            right_ix = palavra_indice[texto_token[i + dist]]\n",
        "            co_occ_mat[ix, right_ix] += 1.0 / dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zT_8ONDWzYl"
      },
      "source": [
        "# Matriz de co-ocorrência\n",
        "co_occ_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZH1cgTUWzYm"
      },
      "source": [
        "# Transposta da matriz de co-ocorrências\n",
        "# Retorna um array 2-D com uma linha para cada elemento não-zero \n",
        "co_occs = np.transpose(np.nonzero(co_occ_mat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYEdxJ7jWzYm"
      },
      "source": [
        "# Print\n",
        "print(\"Shape da Matriz de Co-Ocorrência:\", co_occ_mat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4bwP50xWzYn"
      },
      "source": [
        "# Print\n",
        "print(\"Matriz de Co-Ocorrência Não-Zero:\\n\", co_occs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEvFZF1YWzYo"
      },
      "source": [
        "### Criando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCmpXtwAWzYo"
      },
      "source": [
        "# Tamanho da embedding\n",
        "EMBEDDING_SIZE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-316pnJ0WzYp"
      },
      "source": [
        "# Hiperparâmetros\n",
        "X_MAX = 100\n",
        "ALPHA = 0.75\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.05\n",
        "EPOCHS = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWNJ6YzYWzYq"
      },
      "source": [
        "# Classe para o modelo\n",
        "class Glove(nn.Module):\n",
        "\n",
        "    # Método construtor\n",
        "    def __init__(self, vocab_size, comat, embedding_size, x_max, alpha):\n",
        "        super(Glove, self).__init__()\n",
        "        \n",
        "        # Matriz de embeddings com as palavras centrais\n",
        "        self.embedding_V = nn.Embedding(vocab_size, embedding_size)\n",
        "        \n",
        "        # Matriz de embeddings com as palavras de contexto\n",
        "        self.embedding_U = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        # Bias\n",
        "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
        "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
        "        \n",
        "        # Inicializa os parâmtetros (pesos que a rede aprende durante o treinamento)\n",
        "        for params in self.parameters():\n",
        "            nn.init.uniform_(params, a = -0.5, b = 0.5)\n",
        "            \n",
        "        # Define os hiperparâmetros (que controlam o treinamento)\n",
        "        self.x_max = x_max\n",
        "        self.alpha = alpha\n",
        "        self.comat = comat\n",
        "    \n",
        "    # Função de forward\n",
        "    def forward(self, center_word_lookup, context_word_lookup):\n",
        "        \n",
        "        # Matrizes embedding de pesos para centro e contexto\n",
        "        center_embed = self.embedding_V(center_word_lookup)\n",
        "        target_embed = self.embedding_U(context_word_lookup)\n",
        "\n",
        "        # Matrizes embedding de bias para centro e contexto\n",
        "        center_bias = self.v_bias(center_word_lookup).squeeze(1)\n",
        "        target_bias = self.u_bias(context_word_lookup).squeeze(1)\n",
        "\n",
        "        # Elementos da matriz de co-ocorrência\n",
        "        co_occurrences = torch.tensor([self.comat[center_word_lookup[i].item(), context_word_lookup[i].item()]\n",
        "                                       for i in range(BATCH_SIZE)])\n",
        "        \n",
        "        # Carrega os pesos\n",
        "        weights = torch.tensor([self.weight_fn(var) for var in co_occurrences])\n",
        "\n",
        "        # Funçã de perda\n",
        "        loss = torch.sum(torch.pow((torch.sum(center_embed * target_embed, dim = 1)\n",
        "            + center_bias + target_bias) - torch.log(co_occurrences), 2) * weights)\n",
        "        \n",
        "        return loss\n",
        "       \n",
        "    # Definição do peso\n",
        "    def weight_fn(self, x):\n",
        "        if x < self.x_max:\n",
        "            return (x / self.x_max) ** self.alpha\n",
        "        return 1\n",
        "        \n",
        "    # Soma de V e U como nossos vetores de palavras\n",
        "    def embeddings(self):\n",
        "        return self.embedding_V.weight.data + self.embedding_U.weight.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPMNSXwBWzYr"
      },
      "source": [
        "# Função para gerar um bacth de palavras\n",
        "def gera_batch(model, batch_size = BATCH_SIZE):\n",
        "    \n",
        "    # Extrai uma amostra\n",
        "    sample = np.random.choice(np.arange(len(co_occs)), size = batch_size, replace = False)\n",
        "    \n",
        "    # Listas de vetores\n",
        "    v_vecs_ix, u_vecs_ix = [], []\n",
        "    \n",
        "    # Loop pela amostra para gerar os vetores\n",
        "    for chosen in sample:\n",
        "        ind = tuple(co_occs[chosen])  \n",
        "        \n",
        "        lookup_ix_v = ind[0]\n",
        "        lookup_ix_u = ind[1]\n",
        "        \n",
        "        v_vecs_ix.append(lookup_ix_v)\n",
        "        u_vecs_ix.append(lookup_ix_u) \n",
        "        \n",
        "    return torch.tensor(v_vecs_ix), torch.tensor(u_vecs_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQclFZQ2WzYs"
      },
      "source": [
        "### Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ3L7xMSWzYs"
      },
      "source": [
        "# Função para o treinamento\n",
        "def treina_glove(comat):\n",
        "    \n",
        "    # Lista para os erros\n",
        "    losses = []\n",
        "    \n",
        "    # Cria o modelo Glove\n",
        "    model = Glove(vocab_size, comat, embedding_size = EMBEDDING_SIZE, x_max = X_MAX, alpha = ALPHA)\n",
        "    \n",
        "    # Otimizador\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr = LEARNING_RATE)\n",
        "    \n",
        "    # Loop pelo número de épocas\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        # Erro total\n",
        "        total_loss = 0\n",
        "        \n",
        "        # Número de bacthes\n",
        "        num_batches = int(len(texto_token) / BATCH_SIZE)\n",
        "        \n",
        "        # Loop pelos batches\n",
        "        for batch in tqdm(range(num_batches)):\n",
        "            \n",
        "            # Zera os gradientes do modelo\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # Obtém o bacth de dados\n",
        "            data = gera_batch(model, BATCH_SIZE)\n",
        "            \n",
        "            # Calcula o erro\n",
        "            loss = model(*data)\n",
        "            \n",
        "            # Executa o backpropagation\n",
        "            loss.backward()\n",
        "            \n",
        "            # Otimiza os pesos (aqui é onde ocorre o aprendizado)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Erro total para a epoch\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        # Erros do modelo\n",
        "        losses.append(total_loss)\n",
        "        \n",
        "        # Print da epoch e erro médio do modelo\n",
        "        print('Epoch : %d, Erro Médio : %.02f' % (epoch, np.mean(losses)))\n",
        "        \n",
        "    return model, losses "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efQOLJ0EWzYt"
      },
      "source": [
        "# Executa a função de treinamento e retorna o modelo e os erros\n",
        "model, losses = treina_glove(co_occ_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJSGB-dgWzYv"
      },
      "source": [
        "# Função para o plot do erro durante o treinamento\n",
        "def plot_loss(losses, title):\n",
        "    plt.plot(range(len(losses)), losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Erro')\n",
        "    plt.title(title)\n",
        "    plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-WLEtqzWzYv"
      },
      "source": [
        "# Plot\n",
        "plot_loss(losses, \"Erro de Treinamento do Modelo GloVe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "eryiByqZWzYx"
      },
      "source": [
        "### Testando o Modelo: Similaridade de Palavras, analogias de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24nqA5QLWzYx"
      },
      "source": [
        "# Função que retorna a embedding de uma palavra\n",
        "def get_palavra(palavra, modelo, word_to_ix):\n",
        "    return model.embeddings()[word_to_ix[palavra]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_CzU_D5WzYy"
      },
      "source": [
        "# Função para busca a palavra mais próxima\n",
        "def busca_palavra_similaridade(vec, word_to_ix, n = 10):\n",
        "    all_dists = [(w, torch.dist(vec, get_palavra(w, model, palavra_indice))) for w in palavra_indice]\n",
        "    return sorted(all_dists, key = lambda t: t[1])[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBuLBwmeWzY4"
      },
      "source": [
        "# Gerando o vetor (embedding) de uma palavra \n",
        "vector = get_palavra(\"espaço\", model, palavra_indice)\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBNtrabWzZF"
      },
      "source": [
        "# Busca as palavras similares à palavra \"espaço\"\n",
        "busca_palavra_similaridade(vector, palavra_indice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFB4u-n1WzZG"
      },
      "source": [
        "Observe que a palavra \"espaço\" tem 0 de distância para si mesma. A próxima palavra mais parecida com \"espaço\" é \"universo\" e assim por diante. Quanto menor a distância, mais parecida a palavra. Lembrando que a busca por similaridade é feita com as embeddings treinadas com o modelo GloVe. \n",
        "\n",
        "Mais um exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9zQtjvHWzZH"
      },
      "source": [
        "# Gerando o vetor (embedding) de uma palavra \n",
        "vector = get_palavra(\"solar\", model, palavra_indice)\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qttw4p8KWzZI"
      },
      "source": [
        "# Busca as palavras similares à palavra \"solar\"\n",
        "busca_palavra_similaridade(vector, palavra_indice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06UssGosWzZJ"
      },
      "source": [
        "A distância da palavra \"solar\" para si mesma é 0 e a palavra com maior similaridade é \"energia\" o que faz todo sentido se você leu o texto do Asimov usado para treinar o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzk3MS5WzZJ"
      },
      "source": [
        "### Analogia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLCzlSuZWzZK"
      },
      "source": [
        "Observe na imagem acima que criamos uma \"fórmula\" com 3 palavras visando buscar a quarta palavra, o que é feito por analogia das embeddings (vetores de palavras).\n",
        "\n",
        "Criamos então uma função para buscar a palavra por analogia no formato: \n",
        "\n",
        "palavra1 : palavra2 :: palavra3 : ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf-McsqbWzZK"
      },
      "source": [
        "# Função para busca de palavra por analogia\n",
        "def busca_analogia(p1, p2, p3, n = 5, filtro = True):\n",
        "    \n",
        "    # Print\n",
        "    print('\\n[%s : %s :: %s : ?]' % (p1, p2, p3))\n",
        "   \n",
        "    # p2 - p1 + p3 = p4\n",
        "    closest_words = busca_palavra_similaridade(get_palavra(p2, model, palavra_indice) - \n",
        "                                               get_palavra(p1, model, palavra_indice) + \n",
        "                                               get_palavra(p3, model, palavra_indice), \n",
        "                                               palavra_indice)\n",
        "    \n",
        "    # Vamos excluir as 3 palavras passadas como parâmetro\n",
        "    if filtro:\n",
        "        closest_words = [t for t in closest_words if t[0] not in [p1, p2, p3]]\n",
        "        \n",
        "    for tuple in closest_words[:n]:\n",
        "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbEPhwIyWzZL"
      },
      "source": [
        "# Busca por analogia\n",
        "busca_analogia(\"família\", \"crianças\", \"humano\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CAqQmDDWzZM"
      },
      "source": [
        "E aí estão as palavras que melhor se encaixam na quarta palavra, de acordo com nosso modelo.\n",
        "\n",
        "Quanto maior a distância, menor a similaridade! Treine o modelo com seus próprios textos e experimente a busca por similaridade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-NGI57mAWzZN"
      },
      "source": [
        "# Fim"
      ]
    }
  ]
}